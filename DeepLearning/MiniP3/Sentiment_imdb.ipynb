{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_imdb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9IA_6qGnio_"
      },
      "source": [
        "#Sentiment analysis\n",
        "trains a sentiment analysis model to classify movie reviews as positive or negative, based on the text of the review.\n",
        "\n",
        "We'll use the Large Movie Review Dataset that contains the text of 50,000 movie reviews from the Internet Movie Database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuApFbTSAA-P"
      },
      "source": [
        "##Download and prepare the IMDB dataset\n",
        "Let's download and extract the dataset, then explore the directory structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBzB5p-S8cW5",
        "outputId": "b54fa7f4-7d99-42c9-f138-95255cae8a6b"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "\n",
        "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "\n",
        "# remove unused folders to make it easier to load the data\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joflKDMtpf6G"
      },
      "source": [
        "Next, we will use the text_dataset_from_directory utility to create a labeled tf.data.Dataset.\n",
        "\n",
        "The IMDB dataset has already been divided into train and test, but it lacks a validation set. Let's create a validation set using an 80:20 split of the training data by using the validation_split argument below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8PNelCq8e3d",
        "outputId": "3e8fbf49-6ba6-4741-bbda-b4540ff14235"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 1\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "\n",
        "    seed=seed)\n",
        "\n",
        "class_names = raw_train_ds.class_names\n",
        "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test',\n",
        "    batch_size=batch_size)\n",
        "\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi49FjM1qR5N"
      },
      "source": [
        "##Show a sample of raw data and after processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWXJYllx8gib",
        "outputId": "16ff46ee-fb86-4e2c-b782-58fd108489b5"
      },
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "\n",
        "    print(f'Review: {text_batch.numpy()[0]}')\n",
        "    label = label_batch.numpy()[0]\n",
        "    print(f'Label : {label} ({class_names[label]})')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: b\"The idea ia a very short film with a lot of information. Interesting, entertaining and leaves the viewer wanting more. The producer has produced a short film of excellent quality that cannot be compared to any other short film that I have seen. I have rated this film at the highest possible rating. I also recommend that it is shown to office managers and business people in any establishment. What comes out of it is the fact that people with ideas are never listened to, their voice is never heard. It is a lesson to be learned by any office that wants to go forward. I hope that the produced will produce a second part to this 'idea'. I look forward to viewing the sequence. Once again congrats to Halaqah media in producing a film of excellence and quality with a lesson in mind.\"\n",
            "Label : 1 (pos)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtTS2a_i9grF",
        "outputId": "1504f09e-c875-4e8e-d2dd-3b52cc4e57ca"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n",
        "def preProcess_data(text):\n",
        "   text = text.lower()\n",
        "   new_text = re.sub('[^a-zA-z0-9\\s]','',text)\n",
        "   new_text = re.sub('rt', '', new_text)\n",
        "   return new_text\n",
        "   \n",
        "x_l=[]\n",
        "o_l=[]\n",
        "for text_batch, label_batch in test_ds.take(-1):\n",
        " x_l.append(text_batch.numpy()[0].decode(\"utf-8\"))\n",
        " o_l.append(label_batch.numpy()[0])\n",
        "  \n",
        "print(x_l[1]) \n",
        "print(o_l[0]) "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In England we often feel very attached to British films that we like, as we are so used to the usual American settings and accents. Being from London, where Virtual Sexuality is set, I felt a strong emotional attachment to it. The characters in Virtual Sexuality, particularly the females, are exactly what British teenagers are like, I felt like I was almost in the film. I immediately related to the character of Alex from the film, his shyness is quite common in most British teenage boys, especially around girls. Virtual Sexuality made me feel really good as its one of the only British films that isn't about gangsters or the middle-upper class, but about the people who are watching the film, average teenagers. Americans wouldn't really feel the emotional attachment, but every British teenager should watch it. Anyone from London will recognise the parts of the city from the film, it's definately got a special place in my video box!\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez4_qdAWMb0S",
        "outputId": "732fcc40-1052-4654-b28d-1c5712b23b6f"
      },
      "source": [
        "x_eval=[]\n",
        "o_eval=[]\n",
        "for text_batch, label_batch in test_ds.take(-1):\n",
        " x_eval.append(text_batch.numpy()[0].decode(\"utf-8\"))\n",
        " o_eval.append(label_batch.numpy()[0])\n",
        "  \n",
        "print(x_eval[1]) \n",
        "print(o_eval[0])\n",
        "print(len(x_eval))  "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In England we often feel very attached to British films that we like, as we are so used to the usual American settings and accents. Being from London, where Virtual Sexuality is set, I felt a strong emotional attachment to it. The characters in Virtual Sexuality, particularly the females, are exactly what British teenagers are like, I felt like I was almost in the film. I immediately related to the character of Alex from the film, his shyness is quite common in most British teenage boys, especially around girls. Virtual Sexuality made me feel really good as its one of the only British films that isn't about gangsters or the middle-upper class, but about the people who are watching the film, average teenagers. Americans wouldn't really feel the emotional attachment, but every British teenager should watch it. Anyone from London will recognise the parts of the city from the film, it's definately got a special place in my video box!\n",
            "0\n",
            "25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6UIRS87t3IT"
      },
      "source": [
        "##Tokenize and Save Keras Tokenizer\n",
        ">The tokenizer will transform the text into vectors, it’s important to have the same vector space between training & predicting. The most common way is to save tokenizer and load the same tokenizer at predicting time using pickle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AwkfSsEA7iE"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "\n",
        "max_fatures = 20000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(x_l)\n",
        "X = tokenizer.texts_to_sequences(x_l)\n",
        "X = pad_sequences(X, 200) \n",
        "\n",
        "Y = pd.get_dummies(o_l)\n",
        "\n",
        "\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G79Jus8uXI_"
      },
      "source": [
        "##Split Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQh1JB1WBBI0",
        "outputId": "d2908acc-4775-48fe-c1ad-101d6ba49e0f"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20)\n",
        "\n",
        "print(len(X_train), \"Training sequences\")\n",
        "print(len(X_test), \"Validation sequences\")\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000 Training sequences\n",
            "5000 Validation sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDPb1ZSiPeaG"
      },
      "source": [
        "X_E = tokenizer.texts_to_sequences(x_eval)\n",
        "X_E = pad_sequences(X_E, 200) \n",
        "Y_E = pd.get_dummies(o_eval)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwxI3iNxPbH3",
        "outputId": "a92fa4a3-551e-4cf1-9838-25a8559cdf72"
      },
      "source": [
        "X_eval, X_, Y_eval, Y_ = train_test_split(X_E,Y_E, test_size = 0.00000000001)\n",
        "\n",
        "print(len(Y_eval), \"Testing sequences\")\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24999 Testing sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfpLGQT_7DTF"
      },
      "source": [
        "##Bidirectional LSTM Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6A1Mjlp67J8"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "max_features = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxszyE3X7KFl"
      },
      "source": [
        "Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LlunBAj7JH5",
        "outputId": "368836bf-5f2a-4a57-ea64-a3fc5876f25f"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, 128,input_length = 28))\n",
        "\n",
        "# Add 2 bidirectional LSTMs\n",
        "model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
        "model.add(layers.Bidirectional(layers.LSTM(64)))\n",
        "# Add a classifier\n",
        "model.add(layers.Dense(2, activation=\"sigmoid\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 28, 128)           2560000   \n",
            "_________________________________________________________________\n",
            "bidirectional_8 (Bidirection (None, 28, 128)           98816     \n",
            "_________________________________________________________________\n",
            "bidirectional_9 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 2,757,890\n",
            "Trainable params: 2,757,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96FzUa4FFBZc",
        "outputId": "54c7ddaf-4277-4bd0-8aca-cd2a4e52b6bf"
      },
      "source": [
        "model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(X_train, Y_train, batch_size=512, epochs=4, validation_data=(X_test, Y_test))\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "40/40 [==============================] - 11s 164ms/step - loss: 0.5335 - accuracy: 0.7218 - val_loss: 0.4401 - val_accuracy: 0.8374\n",
            "Epoch 2/4\n",
            "40/40 [==============================] - 5s 134ms/step - loss: 0.2462 - accuracy: 0.9042 - val_loss: 0.2864 - val_accuracy: 0.8826\n",
            "Epoch 3/4\n",
            "40/40 [==============================] - 5s 132ms/step - loss: 0.1266 - accuracy: 0.9571 - val_loss: 0.3012 - val_accuracy: 0.8866\n",
            "Epoch 4/4\n",
            "40/40 [==============================] - 5s 133ms/step - loss: 0.0713 - accuracy: 0.9775 - val_loss: 0.4116 - val_accuracy: 0.8716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f12ce85f990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dymwx9PFNzO",
        "outputId": "feb13f7a-7b42-4aef-f8c6-68b12a388c1c"
      },
      "source": [
        "# importing library\n",
        "\n",
        "print(len(X_eval), \"Testing sequences\")\n",
        "loss, accuracy = model.evaluate(X_eval,Y_eval)\n",
        "\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24999 Testing sequences\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.1228 - accuracy: 0.9637\n",
            "Loss: 0.12278015166521072\n",
            "Accuracy: 0.9636785387992859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCdLsP0YroYL"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkL8GHOstXou"
      },
      "source": [
        "The tokenizer will transform the text into vectors, it’s important to have the same vector space between training & predicting. The most common way is to save tokenizer and load the same tokenizer at predicting time using pickle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxb8R_q6tVjC"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "   loaded_tokenizer = pickle.load(handle)   "
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDL4iFRSrrui",
        "outputId": "7a7a1806-d065-419f-841b-50562969e303"
      },
      "source": [
        "examples = [\n",
        "    'this is such an amazing movie!',  # this is the same sentence tried earlier\n",
        "    'The movie was great!',\n",
        "    'The movie was fine.',\n",
        "    'The movie was bad.',\n",
        "    'The movie was terrible...'\n",
        "]\n",
        "for ex in examples:\n",
        "  txt=preProcess_data(ex)\n",
        "  seq= loaded_tokenizer.texts_to_sequences([txt])\n",
        "  padded = pad_sequences(seq, maxlen=maxlen)\n",
        "  pred = model.predict_classes(padded)\n",
        "  print(txt,pred) \n",
        "  \n",
        "\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "this is such an amazing movie [1]\n",
            "the movie was great [1]\n",
            "the movie was fine [1]\n",
            "the movie was bad [0]\n",
            "the movie was terrible [0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}